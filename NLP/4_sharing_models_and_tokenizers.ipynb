{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers[torch]\n",
        "!pip install --quiet accelerate -U"
      ],
      "metadata": {
        "id": "HLgzKM--TXF0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sharing models and tokenizers"
      ],
      "metadata": {
        "id": "f-d0G9SxTD--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using pretrained models\n",
        "\n",
        "The Model Hub makes selecting the appropriate model simple, so that using it in any downstream library can be done in a few lines of code.\n",
        "\n",
        "We select the camembert-base checkpoint to try it out. The identifier camembert-base is all we need to start using it! As youâ€™ve seen in previous chapters, we can instantiate it using the pipeline() function:"
      ],
      "metadata": {
        "id": "pN3JlFAVTI_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffsSvqG6S-B-",
        "outputId": "f11e5890-03fa-4d8f-d70b-386d5bf3f20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
        "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task itâ€™s going to be used for. For example, here we are loading the camembert-base checkpoint in the fill-mask pipeline, which is completely fine. But if we were to load this checkpoint in the text-classification pipeline, the results would not make any sense because the head of camembert-base is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints.\n",
        "\n",
        "\n",
        "You can also instantiate the checkpoint using the model architecture directly:\n",
        "\n"
      ],
      "metadata": {
        "id": "YsNdYSxaTfio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
        "\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Irt8DA5TkJp",
        "outputId": "414b6f34-00c9-41d0-ef4b-376e76c9a5a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we recommend using the Auto* classes instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the [Auto* classes](https://huggingface.co/docs/transformers/model_doc/auto?highlight=auto#auto-classes) makes switching checkpoints simple:"
      ],
      "metadata": {
        "id": "-XPGLyBRTmDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHJp1ULTzb9",
        "outputId": "a04b3e89-cd16-44ce-a238-8b3100cf7c2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Sharing pretrained models\n",
        "\n",
        " Let's take a look at the easiest ways to share pretrained models to the ðŸ¤— Hub.\n",
        "\n",
        " We encourage all users that train models to contribute by sharing them with the community â€” sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done!\n",
        "\n",
        "There are three ways to go about creating new model repositories:\n",
        "\n",
        "- Using the push_to_hub API\n",
        "- Using the huggingface_hub Python library\n",
        "- Using the web interface\n",
        "\n",
        "Once youâ€™ve created a repository, you can upload files to it via git and git-lfs."
      ],
      "metadata": {
        "id": "iX9dJwMeTzNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the push_to_hub API**\n",
        "\n",
        "Before going further, youâ€™ll need to generate an authentication token so that the huggingface_hub API knows who you are and what namespaces you have write access to. Make sure you are in an environment where you have transformers installed.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "in terminal, you can run:\n",
        "\n",
        "huggingface-cli login\n",
        "\n",
        "```\n",
        "\n",
        "If you have played around with the Trainer API to train a model, the easiest way to upload it to the Hub is to set push_to_hub=True when you define your TrainingArguments:\n",
        "\n"
      ],
      "metadata": {
        "id": "7SJhvS16T8ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"bert-finetuned-mrpc\", save_strategy=\"epoch\", push_to_hub=True\n",
        ")"
      ],
      "metadata": {
        "id": "NpoamS_bT8xd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call `trainer.train()`, the Trainer will save and upload your model to the Hub in a repository within your namespace at each save point (e.g., every epoch). The repository will have the same name as your output directory unless you specify a different name using `hub_model_id = \"a_different_name\"`.\n",
        "\n",
        "To upload your model to an organization you belong to, use `hub_model_id = \"my_organization/my_repo_name\"`.\n",
        "\n",
        "After training, perform a final `trainer.push_to_hub()` to upload the latest version of your model. This will also generate a model card with metadata, hyperparameters, and evaluation results."
      ],
      "metadata": {
        "id": "ULqnjXgLT838"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At a lower level, accessing the Model Hub can be done directly on models, tokenizers, and configuration objects via their push_to_hub() method. This method takes care of both the repository creation and pushing the model and tokenizer files directly to the repository. No manual handling is required, unlike with the API weâ€™ll see below.\n",
        "\n",
        "To get an idea of how it works, letâ€™s first initialize a model and a tokenizer:"
      ],
      "metadata": {
        "id": "IHc5D6ajWRO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsTYc8g3T8-M",
        "outputId": "ff23b38d-73c8-4c14-c4da-2e21db21c5b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing CamembertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Youâ€™re free to do whatever you want with these â€” add tokens to the tokenizer, train the model, fine-tune it. Once youâ€™re happy with the resulting model, weights, and tokenizer, you can leverage the push_to_hub() method directly available on the model object:"
      ],
      "metadata": {
        "id": "kKPcce_3WWq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"dummy-model\")"
      ],
      "metadata": {
        "id": "UNj21BH7XVEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create the new repository dummy-model in your profile, and populate it with your model files. Do the same with the tokenizer, so that all the files are now available in this repository:"
      ],
      "metadata": {
        "id": "hhcKggjHWW_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\")"
      ],
      "metadata": {
        "id": "jm9tM7PDXXFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you belong to an organization, simply specify the organization argument to upload to that organizationâ€™s namespace:"
      ],
      "metadata": {
        "id": "AWUQJsd4WeYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\")"
      ],
      "metadata": {
        "id": "aMZNgk0rWefp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to use a specific Hugging Face token, youâ€™re free to specify it to the push_to_hub() method as well:\n",
        "\n"
      ],
      "metadata": {
        "id": "uHigD4dhWemH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\", use_auth_token=\"<TOKEN>\")"
      ],
      "metadata": {
        "id": "WwvUgLahWesI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now head to the Model Hub to find your newly uploaded model: https://huggingface.co/user-or-organization/dummy-model.\n",
        "\n",
        "The `push_to_hub()` method in the ðŸ¤— Transformers library allows for uploading to a specific repository or organization namespace and using different API tokens. For detailed specifications, refer to the ðŸ¤— Transformers documentation. This method uses the `huggingface_hub` Python package, which provides a direct API to the Hugging Face Hub and integrates with various machine learning libraries, including ðŸ¤— Transformers and allenlp. Integrating this method into your own code or library is straightforward."
      ],
      "metadata": {
        "id": "cJbET2GMWeyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the huggingface_hub Python library**\n",
        "\n",
        "The huggingface_hub Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like getting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositoriesâ€™ content and to integrate the Hub in your projects and libraries.\n",
        "\n",
        "Similarly to using the push_to_hub API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the login command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the ! character if running in Google Colab):"
      ],
      "metadata": {
        "id": "lMLBopfaXqR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface-cli login"
      ],
      "metadata": {
        "id": "vdeTFNxbWe5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The huggingface_hub package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:\n",
        "\n"
      ],
      "metadata": {
        "id": "amMHEJ4qXzu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import (\n",
        "    # User management\n",
        "    login,\n",
        "    logout,\n",
        "    whoami,\n",
        "\n",
        "    # Repository creation and management\n",
        "    create_repo,\n",
        "    delete_repo,\n",
        "    update_repo_visibility,\n",
        "\n",
        "    # And some methods to retrieve/change information about the content\n",
        "    list_models,\n",
        "    list_datasets,\n",
        "    list_metrics,\n",
        "    list_repo_files,\n",
        "    upload_file,\n",
        "    delete_file,\n",
        ")"
      ],
      "metadata": {
        "id": "m_qgxmcMX07f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, it offers the very powerful Repository class to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them.\n",
        "\n",
        "The create_repo method can be used to create a new repository on the hub:"
      ],
      "metadata": {
        "id": "JSdi93PYX3-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"dummy-model\")"
      ],
      "metadata": {
        "id": "lrdJ7QOmX4RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create the repository dummy-model in your namespace. If you like, you can specify which organization the repository should belong to using the organization argument:"
      ],
      "metadata": {
        "id": "Wv3KwfFbX5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"dummy-model\", organization=\"huggingface\")"
      ],
      "metadata": {
        "id": "7xrg-EwjX5cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create the dummy-model repository in the huggingface namespace, assuming you belong to that organization. Other arguments which may be useful are:\n",
        "\n",
        "- private, in order to specify if the repository should be visible from others or not.\n",
        "- token, if you would like to override the token stored in your cache by a given token.\n",
        "- repo_type, if you would like to create a dataset or a space instead of a model. Accepted values are \"dataset\" and \"space\".\n",
        "\n",
        "Once the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled."
      ],
      "metadata": {
        "id": "TnXATRhfX75G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the web interface**\n",
        "\n",
        "The web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more.\n",
        "\n",
        "To create a new repository, visit [huggingface.co/new](huggingface.co/new):"
      ],
      "metadata": {
        "id": "oVQ160KfX8JG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, specify the owner of the repository: this can be either you or any of the organizations youâ€™re affiliated with. If you choose an organization, the model will be featured on the organizationâ€™s page and every member of the organization will have the ability to contribute to the repository.\n",
        "\n",
        "Next, enter your modelâ€™s name. This will also be the name of the repository. Finally, you can specify whether you want your model to be public or private. Private models are hidden from public view."
      ],
      "metadata": {
        "id": "ICFJV80EYQP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uploading the model files**\n",
        "\n",
        "The system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for Git Large File Storage) for larger files.\n",
        "\n",
        "In the next section, we go over three different ways of uploading files to the Hub: through huggingface_hub and through git commands."
      ],
      "metadata": {
        "id": "urxfFqwJYTuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The upload_file approach**\n",
        "\n",
        "Using upload_file does not require git and git-lfs to be installed on your system. It pushes files directly to the ðŸ¤— Hub using HTTP POST requests. A limitation of this approach is that it doesnâ€™t handle files that are larger than 5GB in size. If your files are larger than 5GB, please follow the two other methods detailed below.\n",
        "\n",
        "The API may be used as follows:"
      ],
      "metadata": {
        "id": "SQPBnVSpYZsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    \"<path_to_file>/config.json\",\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=\"<namespace>/dummy-model\",\n",
        ")"
      ],
      "metadata": {
        "id": "v-KJch-qX8Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will upload the file config.json available at <path_to_file> to the root of the repository as config.json, to the dummy-model repository. Other arguments which may be useful are:\n",
        "\n",
        "- token, if you would like to override the token stored in your cache by a given token.\n",
        "- repo_type, if you would like to upload to a dataset or a space instead of a model. Accepted values are \"dataset\" and \"space\"."
      ],
      "metadata": {
        "id": "N-0kz-v_YeFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Repository class**\n",
        "\n",
        "The Repository class manages a local repository in a git-like manner. It abstracts most of the pain points one may have with git to provide all features that we require.\n",
        "\n",
        "Using this class requires having git and git-lfs installed, so make sure you have git-lfs installed (see [here](https://git-lfs.com/) for installation instructions) and set up before you begin.\n",
        "\n",
        "In order to start playing around with the repository we have just created, we can start by initialising it into a local folder by cloning the remote repository:"
      ],
      "metadata": {
        "id": "SQu5YL-GYhXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "repo = Repository(\"<path_to_dummy_folder>\", clone_from=\"<namespace>/dummy-model\")"
      ],
      "metadata": {
        "id": "5Ir62KkNX8RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This created the folder <path_to_dummy_folder> in our working directory. This folder only contains the .gitattributes file as thatâ€™s the only file created when instantiating the repository through create_repo.\n",
        "\n",
        "From this point on, we may leverage several of the traditional git methods:"
      ],
      "metadata": {
        "id": "FGahvmnUYq3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo.git_pull()\n",
        "repo.git_add()\n",
        "repo.git_commit()\n",
        "repo.git_push()\n",
        "repo.git_tag()"
      ],
      "metadata": {
        "id": "QDdwwXJaYq_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And others! We recommend taking a look at the Repository documentation available here for an overview of all available methods.\n",
        "\n",
        "At present, we have a model and a tokenizer that we would like to push to the hub. We have successfully cloned the repository, we can therefore save the files within that repository.\n",
        "\n",
        "We first make sure that our local clone is up to date by pulling the latest changes:"
      ],
      "metadata": {
        "id": "Vouv2OpPYvt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo.git_pull()"
      ],
      "metadata": {
        "id": "_eFpPsVNYvzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once that is done, we save the model and tokenizer files:"
      ],
      "metadata": {
        "id": "ywhDjMnuYyN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ],
      "metadata": {
        "id": "m2G6mVwqYyTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <path_to_dummy_folder> now contains all the model and tokenizer files. We follow the usual git workflow by adding files to the staging area, committing them and pushing them to the hub:"
      ],
      "metadata": {
        "id": "sCAUdn4mYyeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo.git_add()\n",
        "repo.git_commit(\"Add model and tokenizer files\")\n",
        "repo.git_push()"
      ],
      "metadata": {
        "id": "kGE51ourYykC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You just pushed your first files on the hub.\n",
        "\n"
      ],
      "metadata": {
        "id": "uDwiJvcXY3x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The git-based approach**\n",
        "This is the very barebones approach to uploading files: weâ€™ll do so with git and git-lfs directly. Most of the difficulty is abstracted away by previous approaches, but there are a few caveats with the following method so weâ€™ll follow a more complex use-case.\n",
        "\n",
        "Using this class requires having git and git-lfs installed, so make sure you have git-lfs installed (see here for installation instructions) and set up before you begin.\n",
        "\n",
        "First start by initializing git-lfs:"
      ],
      "metadata": {
        "id": "jzxzbmsxY5A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git lfs install\n",
        "\n",
        "Updated git hooks.\n",
        "Git LFS initialized."
      ],
      "metadata": {
        "id": "SSlrG35lY9EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once thatâ€™s done, the first step is to clone your model repository:\n",
        "\n",
        "```\n",
        "git clone https://huggingface.co/<namespace>/<your-model-id>\n",
        "```\n",
        "\n",
        "My username is lysandre and Iâ€™ve used the model name dummy, so for me the command ends up looking like the following:\n",
        "\n",
        "```\n",
        "git clone https://huggingface.co/lysandre/dummy\n",
        "```\n",
        "\n",
        "I now have a folder named dummy in my working directory. I can cd into the folder and have a look at the contents:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "cd dummy && ls\n",
        "```\n",
        "\n",
        "If you just created your repository using Hugging Face Hubâ€™s create_repo method, this folder should only contain a hidden .gitattributes file. If you followed the instructions in the previous section to create a repository using the web interface, the folder should contain a single README.md file alongside the hidden .gitattributes file, as shown here.\n",
        "\n",
        "Adding a regular-sized file, such as a configuration file, a vocabulary file, or basically any file under a few megabytes, is done exactly as one would do it in any git-based system. However, bigger files must be registered through git-lfs in order to push them to huggingface.co.\n",
        "\n",
        "Letâ€™s go back to Python for a bit to generate a model and tokenizer that weâ€™d like to commit to our dummy repository:"
      ],
      "metadata": {
        "id": "OKPujDXiY_RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Do whatever with the model, train it, fine-tune it...\n",
        "\n",
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ],
      "metadata": {
        "id": "w9pTe4XjZM3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that weâ€™ve saved some model and tokenizer artifacts, letâ€™s take another look at the dummy folder:\n",
        "\n",
        "```\n",
        "ls\n",
        "```\n",
        "\n",
        "If you look at the file sizes (for example, with ls -lh), you should see that the model state dict file (pytorch_model.bin) is the only outlier, at more than 400 MB.\n",
        "\n",
        "We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Gitâ€™s staging environment using the git add command:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "git add .\n",
        "# We can then have a look at the files that are currently staged:\n",
        "git status\n",
        "\n",
        "```\n",
        "\n",
        "Similarly, we can make sure that git-lfs is tracking the correct files by using its status command:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "git lfs status\n",
        "```\n",
        "\n",
        "We can see that all files have Git as a handler, except pytorch_model.bin and sentencepiece.bpe.model, which have LFS. Great!\n",
        "\n",
        "Letâ€™s proceed to the final steps, committing and pushing to the huggingface.co remote repository:\n",
        "\n",
        "```\n",
        "git commit -m \"First model version\"\n",
        "```\n",
        "\n",
        "Pushing can take a bit of time, depending on the speed of your internet connection and the size of your files:\n",
        "\n",
        "```\n",
        "git push\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c5V9OKV-ZPAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Building a model card\n",
        "\n",
        "A model card is a crucial document in a model repository, as important as the model and tokenizer files. It provides a comprehensive definition of the model, ensuring its reusability by others, reproducibility of results, and a foundation for further development. By documenting the training and evaluation processes, it helps others understand the model's capabilities and limitations, including biases and contexts in which it is useful or not.\n",
        "\n",
        "Creating a model card involves writing a detailed README.md file in Markdown format. The concept of model cards was introduced by Google in the paper \"Model Cards for Model Reporting\" by Margaret Mitchell et al., emphasizing their importance for reproducibility, reusability, and fairness.\n",
        "\n",
        "A typical model card includes:\n",
        "- **Model description**\n",
        "- **Intended uses & limitations**\n",
        "- **How to use**\n",
        "- **Limitations and bias**\n",
        "- **Training data**\n",
        "- **Training procedure**\n",
        "- **Evaluation results**\n",
        "\n",
        "Each section provides essential details about the model's purpose, usage, and performance."
      ],
      "metadata": {
        "id": "W9uVzkxGTzQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model description**\n",
        "\n",
        "The model description provides basic details about the model. This includes the architecture, version, if it was introduced in a paper, if an original implementation is available, the author, and general information about the model. Any copyright should be attributed here. General information about training procedures, parameters, and important disclaimers can also be mentioned in this section."
      ],
      "metadata": {
        "id": "9i-bsO7XUKBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intended uses & limitations**\n",
        "\n",
        "Here you describe the use cases the model is intended for, including the languages, fields, and domains where it can be applied. This section of the model card can also document areas that are known to be out of scope for the model, or where it is likely to perform suboptimally.\n",
        "\n",
        "**How to use**\n",
        "\n",
        "This section should include some examples of how to use the model. This can showcase usage of the pipeline() function, usage of the model and tokenizer classes, and any other code you think might be helpful.\n",
        "\n",
        "**Training data**\n",
        "This part should indicate which dataset(s) the model was trained on. A brief description of the dataset(s) is also welcome.\n",
        "\n",
        "**Training procedure**\n",
        "\n",
        "In this section you should describe all the relevant aspects of training that are useful from a reproducibility perspective. This includes any preprocessing and postprocessing that were done on the data, as well as details such as the number of epochs the model was trained for, the batch size, the learning rate, and so on.\n",
        "\n",
        "**Variable and metrics**\n",
        "\n",
        "Here you should describe the metrics you use for evaluation, and the different factors you are mesuring. Mentioning which metric(s) were used, on which dataset and which dataset split, makes it easy to compare you modelâ€™s performance compared to that of other models. These should be informed by the previous sections, such as the intended users and use cases.\n",
        "\n",
        "**Evaluation results**\n",
        "\n",
        "Finally, provide an indication of how well the model performs on the evaluation dataset. If the model uses a decision threshold, either provide the decision threshold used in the evaluation, or provide details on evaluation at different thresholds for the intended uses.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Check out the following for a few examples of well-crafted model cards:\n",
        "\n",
        "- bert-base-cased\n",
        "- gpt2\n",
        "- distilbert\n",
        "\n",
        "More examples from different organizations and companies are available here.\n",
        "\n",
        "**Note**\n",
        "\n",
        "Model cards are not a requirement when publishing models, and you donâ€™t need to include all of the sections described above when you make one. However, explicit documentation of the model can only benefit future users, so we recommend that you fill in as many of the sections as possible to the best of your knowledge and ability.\n",
        "\n",
        "**Model card metadata**\n",
        "\n",
        "If you have done a little exploring of the Hugging Face Hub, you should have seen that some models belong to certain categories: you can filter them by tasks, languages, libraries, and more. The categories a model belongs to are identified according to the metadata you add in the model card header.\n",
        "\n",
        "For example, if you take a look at the [camembert-base model card](https://huggingface.co/almanach/camembert-base/blob/main/README.md), you should see the following lines in the model card header:\n",
        "\n",
        "```\n",
        "language: fr\n",
        "license: mit\n",
        "datasets:\n",
        "- oscar\n",
        "```\n",
        "\n",
        "This metadata is parsed by the Hugging Face Hub, which then identifies this model as being a French model, with an MIT license, trained on the Oscar dataset.\n",
        "\n",
        "The [full model card specification](https://github.com/huggingface/hub-docs/blame/main/modelcard.md) allows specifying languages, licenses, tags, datasets, metrics, as well as the evaluation results the model obtained when training."
      ],
      "metadata": {
        "id": "s3OcfHnxUM-I"
      }
    }
  ]
}