{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on exercise\n",
        "\n",
        "In this Unit, we consolidated the material covered in the previous six units of the course to build three integrated audio applications. As youâ€™ve experienced, building more involved audio tools is fully within reach by using the foundational skills youâ€™ve acquired in this course.\n",
        "\n",
        "The hands-on exercise takes one of the applications covered in this Unit, and extends it with a few multilingual tweaks ðŸŒ Your objective is to take the [cascaded speech-to-speech translation Gradio demo](https://huggingface.co/spaces/course-demos/speech-to-speech-translation) from the first section in this Unit, and update it to translate to any **non-English** language. That is to say, the demo should take speech in language X, and translate it to speech in language Y, where the target language Y is not English. You should start by [duplicating](https://huggingface.co/spaces/course-demos/speech-to-speech-translation?duplicate=true) the template under your Hugging Face namespace. Thereâ€™s no requirement to use a GPU accelerator device - the free CPU tier works just fine ðŸ¤— However, you should ensure that the visibility of your demo is set to **public**. This is required such that your demo is accessible to us and can thus be checked for correctness.\n",
        "\n",
        "Tips for updating the speech translation function to perform multilingual speech translation are provided in the section on [speech-to-speech translation](https://huggingface.co/learn/audio-course/chapter7/speech-to-speech). By following these instructions, you should be able to update the demo to translate from speech in language X to text in language Y, which is half of the task!\n",
        "\n",
        "To synthesise from text in language Y to speech in language Y, where Y is a multilingual language, you will need to use a multilingual TTS checkpoint. For this, you can either use the SpeechT5 TTS checkpoint that you fine-tuned in the previous hands-on exercise, or a pre-trained multilingual TTS checkpoint. There are two options for pre-trained checkpoints, either the checkpoint sanchit-gandhi/speecht5_tts_vox_nl, which is a SpeechT5 checkpoint fine-tuned on the Dutch split of the VoxPopuli dataset, or an MMS TTS checkpoint (see section on [pretrained models for TTS](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models)).\n",
        "\n",
        "> In our experience experimenting with the Dutch language, using an MMS TTS checkpoint results in better performance than a fine-tuned SpeechT5 one, but you might find that your fine-tuned TTS checkpoint is preferable in your language. If you decide to use an MMS TTS checkpoint, you will need to update the [requirements.txt](https://huggingface.co/spaces/course-demos/speech-to-speech-translation/blob/a03175878f522df7445290d5508bfb5c5178f787/requirements.txt#L2) file of your demo to install transformers from the PR branch:\n",
        "```git+https://github.com/hollance/transformers.git@6900e8ba6532162a8613d2270ec2286c3f58f57```\n",
        "\n",
        "Your demo should take as input an audio file, and return as output another audio file, matching the signature of the [speech_to_speech_translation](https://huggingface.co/spaces/course-demos/speech-to-speech-translation/blob/3946ba6705a6632a63de8672ac52a482ab74b3fc/app.py#L35) function in the template demo. Therefore, we recommend that you leave the main function speech_to_speech_translation as is, and only update the [translate](https://huggingface.co/spaces/course-demos/speech-to-speech-translation/blob/a03175878f522df7445290d5508bfb5c5178f787/app.py#L24) and [synthesis](https://huggingface.co/spaces/course-demos/speech-to-speech-translation/blob/a03175878f522df7445290d5508bfb5c5178f787/app.py#L29) functions as required.\n",
        "\n",
        "Once you have built your demo as a Gradio demo on the Hugging Face Hub, you can submit it for assessment. Head to the Space [audio-course-u7-assessment](https://huggingface.co/spaces/huggingface-course/audio-course-u7-assessment) and provide the repository id of your demo when prompted. This Space will check that your demo has been built correctly by sending a sample audio file to your demo and checking that the returned audio file is indeed non-English. If your demo works correctly, youâ€™ll get a green tick next to your name on the overall [progress space](https://huggingface.co/spaces/MariaK/Check-my-progress-Audio-Course) âœ…"
      ],
      "metadata": {
        "id": "iV_JI3Tahyo7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoL24jSRhwG2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supplemental reading and resources\n",
        "\n",
        "This Unit pieced together many components from previous units, introducing the tasks of speech-to-speech translation, voice assistants and speaker diarization. The supplemental reading material is thus split into these three new tasks for your convenience:\n",
        "\n",
        "Speech-to-speech translation:\n",
        "\n",
        "- [STST with discrete units](https://ai.meta.com/blog/advancing-direct-speech-to-speech-modeling-with-discrete-units/) by Meta AI: a direct approach to STST through encoder-decoder models\n",
        "- [Hokkien direct speech-to-speech translation](https://ai.meta.com/blog/ai-translation-hokkien/) by Meta AI: a direct approach to STST using encoder-decoder models with a two-stage decoder\n",
        "- [Leveraging unsupervised and weakly-supervised data to improve direct STST](https://arxiv.org/abs/2203.13339) by Google: proposes new approaches for leveraging unsupervised and weakly supervised data for training direct STST models and a small change to the Transformer architecture\n",
        "- [Translatotron-2](https://google-research.github.io/lingvo-lab/translatotron2/) by Google: a system that is able to retain speaker characteristics in translated speech\n",
        "\n",
        "Voice Assistant:\n",
        "\n",
        "- [Accurate wakeword detection](https://www.amazon.science/publications/accurate-detection-of-wake-word-start-and-end-using-a-cnn) by Amazon: a low latency approach for wakeword detection for on-device applications\n",
        "- [RNN-Transducer Architecture](https://arxiv.org/pdf/1811.06621) by Google: a modification to the CTC architecture for streaming on-device ASR\n",
        "\n",
        "Meeting Transcriptions:\n",
        "- pyannote.audio Technical Report by HervÃ© Bredin: this report describes the main principles behind the pyannote.audio speaker diarization pipeline\n",
        "- [Whisper X](https://arxiv.org/pdf/2303.00747) by Max Bain et al.: a superior approach to computing word-level timestamps using the Whisper model"
      ],
      "metadata": {
        "id": "vZrjfOMvrPk_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndBmbXIMs5A-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}