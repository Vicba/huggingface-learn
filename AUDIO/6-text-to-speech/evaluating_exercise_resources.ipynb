{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating text-to-speech models\n",
        "\n",
        "During the training time, text-to-speech models optimize for the mean-square error loss (or mean absolute error) between the predicted spectrogram values and the generated ones. Both MSE and MAE encourage the model to minimize the difference between the predicted and target spectrograms. However, since TTS is a one-to-many mapping problem, i.e. the output spectrogram for a given text can be represented in many different ways, the evaluation of the resulting text-to-speech (TTS) models is much more difficult.\n",
        "\n",
        "Unlike many other computational tasks that can be objectively measured using quantitative metrics, such as accuracy or precision, evaluating TTS relies heavily on subjective human analysis.\n",
        "\n",
        "One of the most commonly employed evaluation methods for TTS systems is conducting qualitative assessments using mean opinion scores (MOS). MOS is a subjective scoring system that allows human evaluators to rate the perceived quality of synthesized speech on a scale from 1 to 5. These scores are typically gathered through listening tests, where human participants listen to and rate the synthesized speech samples.\n",
        "\n",
        "One of the main reasons why objective metrics are challenging to develop for TTS evaluation is the subjective nature of speech perception. Human listeners have diverse preferences and sensitivities to various aspects of speech, including pronunciation, intonation, naturalness, and clarity. Capturing these perceptual nuances with a single numerical value is a daunting task. At the same time, the subjectivity of the human evaluation makes it challenging to compare and benchmark different TTS systems.\n",
        "\n",
        "Furthermore, this kind of evaluation may overlook certain important aspects of speech synthesis, such as naturalness, expressiveness, and emotional impact. These qualities are difficult to quantify objectively but are highly relevant in applications where the synthesized speech needs to convey human-like qualities and evoke appropriate emotional responses.\n",
        "\n",
        "In summary, evaluating text-to-speech models is a complex task due to the absence of one truly objective metric. The most common evaluation method, mean opinion scores (MOS), relies on subjective human analysis. While MOS provides valuable insights into the quality of synthesized speech, it also introduces variability and subjectivity."
      ],
      "metadata": {
        "id": "PwYWWnW4B3xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hands-on exercise\n",
        "\n",
        "In this unit, we have explored text-to-speech audio task, talked about existing datasets, pretrained models and nuances of fine-tuning SpeechT5 for a new language.\n",
        "\n",
        "As you’ve seen, fine-tuning models for text-to-speech task can be challenging in low-resource scenarios. At the same time, evaluating text-to-speech models isn’t easy either.\n",
        "\n",
        "For these reasons, this hands-on exercise will focus on practicing the skills rather than achieving a certain metric value.\n",
        "\n",
        "Your objective for this task is to fine-tune SpeechT5 on a dataset of your choosing. You have the freedom to select another language from the same voxpopuli dataset, or you can pick any other dataset listed in this unit.\n",
        "\n",
        "Be mindful of the training data size! For training on a free tier GPU from Google Colab, we recommend limiting the training data to about 10-15 hours.\n",
        "\n",
        "Once you have completed the fine-tuning process, share your model by uploading it to the Hub. Make sure to tag your model as a text-to-speech model either with appropriate kwargs, or in the Hub UI.\n",
        "\n",
        "Remember, the primary aim of this exercise is to provide you with ample practice, allowing you to refine your skills and gain a deeper understanding of text-to-speech audio tasks."
      ],
      "metadata": {
        "id": "0POwklaZB8rN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Supplemental reading and resources\n",
        "\n",
        "This unit introduced the text-to-speech task, and covered a lot of ground. Want to learn more? Here you will find additional resources that will help you deepen your understanding of the topics and enhance your learning experience.\n",
        "\n",
        "- [HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis:](https://arxiv.org/pdf/2010.05646) a paper introducing HiFi-GAN for speech synthesis.\n",
        "- [X-Vectors:](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf) Robust DNN Embeddings For Speaker Recognition: a paper introducing X-Vector method for speaker embeddings.\n",
        "- [FastSpeech 2:](https://arxiv.org/pdf/2006.04558) Fast and High-Quality End-to-End Text to Speech: a paper introducing FastSpeech 2, another popular text-to-speech model that uses a non-autoregressive TTS method.\n",
        "- [A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech:](https://arxiv.org/pdf/2302.04215v1) a paper introducing MQTTS, an autoregressive TTS system that replaces mel-spectrograms with quantized discrete representation."
      ],
      "metadata": {
        "id": "CnQ27jbDCAl-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTqwcmTGBvZC"
      },
      "outputs": [],
      "source": []
    }
  ]
}