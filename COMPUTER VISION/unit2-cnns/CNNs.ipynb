{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## VGG\n",
        "\n",
        "The VGG architecture was developed in 2014 by Karen Simonyan and Andrew Zisserman from the Visual Geometry Group -and hence named VGG- at Oxford University. The model demonstrated significant improvements over the past models at that time- to be specific 2014 Imagenet challange also known as ILSVRC.\n",
        "\n"
      ],
      "metadata": {
        "id": "iCWC4YTGxfb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "1rMv1SUExazh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKNhXH9Fo7rO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class VGG19(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(VGG19, self).__init__()\n",
        "\n",
        "        # Feature extraction layers: Convolutional and pooling layers\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                3, 64, kernel_size=3, padding=1\n",
        "            ),  # 3 input channels, 64 output channels, 3x3 kernel, 1 padding\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(\n",
        "                kernel_size=2, stride=2\n",
        "            ),  # Max pooling with 2x2 kernel and stride 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                512 * 7 * 7, 4096\n",
        "            ),  # 512 channels, 7x7 spatial dimensions after max pooling\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Dropout layer with 0.5 dropout probability\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes),  # Output layer with 'num_classes' output units\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)  # Pass input through the feature extractor layers\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.classifier(x)  # Pass flattened output through the classifier layers\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### timm\n",
        "\n",
        "timm (or PyTorch Image Models) is a Python library that provides a collection of pre-trained deep learning models, primarily focused on computer vision tasks, along with utilities for training, fine-tuning, and inference.\n",
        "\n"
      ],
      "metadata": {
        "id": "MdKWzHF5zdyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CsXksLyxlGj",
        "outputId": "71256b07-7154-413b-fa25-decb72dcd3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "\n",
        "# Load a pre-trained MobileNet model\n",
        "model_name = \"mobilenetv3_large_100\"\n",
        "\n",
        "model = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "# If you want to use the model for inference\n",
        "model.eval()\n",
        "\n",
        "# Forward pass with a dummy input\n",
        "# Batch size 1, 3 color channels, 224x224 image\n",
        "input_tensor = torch.rand(1, 3, 224, 224)\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output, output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAQFWQ36zn9B",
        "outputId": "56224e9f-ed76-4860-cbd1-5b3f54782a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.7906e+00,  9.9168e-02,  4.3141e-01, -2.1836e-02,  7.9626e-01,\n",
            "         -1.2775e-01,  4.4802e-01, -9.9817e-02, -7.1898e-03, -4.3294e-01,\n",
            "          3.5207e-01,  7.1959e-01,  1.3107e+00,  6.6033e-01,  9.0555e-01,\n",
            "          1.0772e+00,  1.7348e+00, -6.7611e-03,  2.0565e+00,  1.3547e+00,\n",
            "          6.1916e-01,  3.6509e+00,  2.3372e+00,  3.2081e+00,  9.6124e-01,\n",
            "         -2.3309e-01, -1.2464e+00, -6.4247e-01, -8.4977e-01, -1.7333e+00,\n",
            "         -7.3374e-01, -8.0598e-01, -1.6248e+00,  9.5085e-01,  3.4862e+00,\n",
            "         -1.3481e+00, -9.9721e-01, -1.0850e+00,  1.5498e-02, -7.8427e-01,\n",
            "         -3.4210e-01,  9.9255e-03,  1.8521e+00,  6.5071e-01,  2.3228e-01,\n",
            "          3.6012e-01, -7.1645e-02, -2.5739e-01, -6.5575e-01, -1.5350e+00,\n",
            "          1.2933e-01,  1.3116e-01, -5.1844e-02,  4.5994e-01,  1.3465e+00,\n",
            "         -2.3955e-02,  5.6554e-01, -1.0697e+00,  1.8011e+00, -4.9006e-01,\n",
            "          1.6897e+00, -6.2983e-02,  2.9604e-01,  2.8298e-01, -2.5532e-01,\n",
            "          3.1485e+00,  1.8067e+00,  6.7258e-01,  2.2186e+00,  1.2076e+00,\n",
            "          3.4964e-01,  1.7049e+00,  2.7148e-01, -3.5008e-01,  3.2244e-03,\n",
            "          6.2497e-01,  5.0840e-01,  9.3881e-01,  2.8768e+00,  1.9660e+00,\n",
            "         -2.9265e-01,  6.6771e-01, -1.6628e-01, -7.4501e-01, -1.0139e-01,\n",
            "          1.1911e+00, -2.8326e-01,  1.5753e+00,  1.1466e+00,  2.0907e+00,\n",
            "         -5.2358e-01,  1.3079e+00,  3.2169e+00,  9.1869e-01,  3.2448e+00,\n",
            "          2.8246e-01,  1.1836e+00,  1.5212e-01,  6.9808e-01,  1.3806e+00,\n",
            "         -9.0134e-02, -1.1134e+00,  4.7556e-01, -3.7975e-01, -1.1473e-01,\n",
            "         -6.5288e-02,  3.5149e-01,  2.3154e+00, -5.4977e-01,  1.4851e-01,\n",
            "         -1.0632e+00,  1.3640e+00,  2.4721e+00, -6.1512e-03,  9.7444e-01,\n",
            "         -1.6151e+00, -1.3943e+00,  9.6728e-01, -6.8935e-01,  5.5353e-02,\n",
            "          1.2262e+00, -1.1764e+00, -1.8303e+00, -1.3797e+00, -5.6515e-01,\n",
            "          1.6521e+00,  2.8630e-01,  2.3870e+00,  2.8510e+00,  1.9445e+00,\n",
            "         -3.3249e-01,  4.8664e-01,  1.8720e+00,  1.4354e+00,  2.9029e-01,\n",
            "          5.1055e-01, -7.0991e-02,  1.3963e+00,  1.6526e+00,  1.3815e+00,\n",
            "          1.1412e+00,  1.0404e+00,  8.7302e-01,  8.6459e-01, -5.1808e-02,\n",
            "         -1.2497e+00,  1.9138e+00,  5.2189e-01,  1.5158e-01, -1.4035e+00,\n",
            "         -6.3434e-01,  8.6153e-01, -1.0753e+00,  6.0863e-01, -7.0316e-02,\n",
            "          1.4626e+00,  1.7198e-01,  2.6808e-02,  4.5481e-01, -9.6395e-01,\n",
            "         -1.8726e+00,  3.3963e-01,  1.0538e+00, -1.3141e-01,  1.2083e+00,\n",
            "         -5.1560e-01,  5.6703e-01, -1.1442e+00,  1.1973e+00, -5.7142e-01,\n",
            "          1.3104e-01,  6.9837e-01, -4.6553e-01, -5.1243e-01,  3.2740e-01,\n",
            "         -1.1312e+00, -2.2383e+00, -1.8544e+00,  8.8089e-02, -2.0571e+00,\n",
            "          7.5999e-01, -1.2446e+00, -2.1726e-01, -8.6078e-01,  1.4575e-01,\n",
            "         -5.1013e-01,  4.2097e-02,  1.4734e+00, -1.3861e+00, -7.4229e-01,\n",
            "         -1.3210e-01, -2.3568e+00,  5.4927e-01, -5.0940e-01, -6.3501e-01,\n",
            "          3.9566e-02,  6.8787e-01, -7.3956e-01,  6.8643e-02, -9.4572e-01,\n",
            "         -6.7583e-01,  8.9027e-02, -1.8792e-01,  1.0140e+00, -1.0747e+00,\n",
            "         -1.1299e+00, -9.7144e-01, -4.0666e-01, -1.1631e+00,  1.1265e-01,\n",
            "         -3.7037e-01,  2.0458e-02, -4.7473e-01, -9.1057e-01, -8.9556e-01,\n",
            "         -1.4206e-01, -8.3102e-01, -3.2043e-01,  2.0711e-03, -4.8095e-01,\n",
            "         -7.5547e-01, -1.3660e+00, -9.6944e-01,  2.2335e-01, -1.2487e+00,\n",
            "         -7.1460e-01, -1.1007e+00,  9.0432e-01, -7.5863e-01,  4.2055e-01,\n",
            "         -1.6858e+00, -7.0924e-01, -3.4703e-01, -1.2315e+00, -8.4641e-01,\n",
            "          2.1301e-01, -2.1359e-02,  5.8884e-01,  9.0869e-01, -1.4257e+00,\n",
            "         -1.2791e+00, -7.0595e-01, -1.2657e+00, -1.0781e+00, -1.6342e+00,\n",
            "          2.3805e-01, -1.4683e+00, -5.2509e-01,  3.1320e-01, -1.1244e-02,\n",
            "         -6.0726e-01, -4.0395e-01, -1.1431e+00,  2.9838e-01, -1.1911e+00,\n",
            "         -1.5440e+00, -1.8016e-01,  4.8874e-02,  5.6366e-01,  3.8009e-01,\n",
            "         -6.0622e-01, -4.5688e-01, -2.2007e+00,  3.1838e-01,  1.0048e+00,\n",
            "          7.9937e-01, -5.3602e-01, -7.5909e-01, -1.0938e+00, -1.0876e+00,\n",
            "         -7.3790e-01, -1.5219e+00,  7.7195e-02,  4.1310e-01, -1.0419e+00,\n",
            "         -9.3458e-03,  1.6102e-01, -2.9020e-01,  6.5748e-02, -6.8359e-01,\n",
            "         -4.8085e-01,  8.7125e-01,  5.8280e-02, -9.9169e-01,  7.9558e-01,\n",
            "          2.5690e+00, -6.5803e-01, -5.8819e-01, -1.9116e+00, -2.2583e+00,\n",
            "         -1.1841e+00, -4.2279e-01, -1.8948e+00,  9.1918e-02, -1.9854e+00,\n",
            "         -1.8276e-01, -1.0353e+00,  2.1991e-01,  6.8891e-01,  1.4068e+00,\n",
            "         -3.6984e-01,  1.4739e+00, -5.4224e-01, -5.4562e-01,  3.2697e-01,\n",
            "          3.0747e+00,  1.2605e+00,  3.9197e-01, -5.1785e-01, -9.8372e-01,\n",
            "          1.0317e+00,  1.7159e-01,  1.3410e+00,  1.6200e+00,  2.2715e+00,\n",
            "          3.4623e-01,  1.3136e-01, -1.1914e-01,  9.6644e-01,  1.2337e+00,\n",
            "          6.2039e-01, -1.5521e+00, -1.2509e+00, -3.6459e-01, -5.4938e-01,\n",
            "         -2.1047e-01, -9.7200e-01,  2.8653e+00,  7.9770e-01,  6.1244e-01,\n",
            "         -8.4156e-01, -1.7574e-01, -3.4263e-01, -2.4242e+00,  1.6324e-01,\n",
            "         -7.8884e-01, -9.0710e-01, -1.7547e+00, -1.6067e+00, -7.7821e-01,\n",
            "         -1.8498e+00, -1.9634e+00, -1.6672e+00, -6.1426e-01, -1.9102e+00,\n",
            "         -1.6722e+00, -3.3572e-01, -8.2686e-01, -3.5851e-01,  1.0574e+00,\n",
            "         -2.7482e-02, -3.4217e-01, -1.2725e+00,  1.2497e+00,  3.5358e-01,\n",
            "         -1.8355e+00,  1.2387e+00, -9.9021e-01,  2.7778e-01,  1.7413e+00,\n",
            "         -1.2397e+00,  1.4844e-01, -2.0907e-01,  9.5172e-01, -3.3637e-01,\n",
            "         -3.0396e-01, -7.4603e-01, -1.9469e-01, -1.0158e+00, -1.0856e+00,\n",
            "         -1.2276e+00, -1.7479e+00, -2.4469e-01, -6.6377e-01, -7.1841e-01,\n",
            "         -4.7349e-01,  5.7600e-01, -1.4986e-01, -9.8129e-01, -9.7581e-01,\n",
            "         -5.8513e-01, -5.9275e-01, -1.8082e-01, -8.8020e-01,  1.3100e-01,\n",
            "         -1.3012e+00, -1.1171e+00, -1.0489e+00, -1.1893e+00, -1.3734e+00,\n",
            "         -3.7784e-01, -1.0533e+00, -2.2205e+00, -1.9057e+00, -1.3533e+00,\n",
            "         -5.5634e-02, -1.1740e+00,  1.5330e+00, -1.3364e+00,  1.7023e-01,\n",
            "         -1.0201e+00, -1.2133e+00,  1.0055e+00, -1.1229e+00,  7.7630e-01,\n",
            "          2.2922e+00, -1.0209e+00, -8.8100e-01, -2.3396e+00,  1.7782e+00,\n",
            "         -1.5814e+00,  1.1163e+00,  5.2866e-01,  5.0887e-01,  3.1354e-01,\n",
            "         -7.0373e-01,  3.8524e-01,  1.2153e+00,  2.3733e+00,  5.9532e-01,\n",
            "          2.6666e+00,  4.5376e-01, -1.5149e+00, -9.5742e-01, -2.0206e+00,\n",
            "         -1.4171e-01,  1.1670e-01, -1.6341e+00, -1.0095e+00,  9.0372e-01,\n",
            "         -9.6122e-01, -4.0500e-01,  1.4824e+00,  6.6581e-01,  4.5886e-02,\n",
            "         -2.2026e+00, -1.5753e+00,  1.5921e+00, -1.4809e-01, -3.8035e-01,\n",
            "          1.0004e+00, -5.6742e-01,  3.0361e-01,  1.6240e+00, -4.0029e-01,\n",
            "         -7.6136e-01,  4.2598e+00, -1.4089e+00,  7.7571e-01, -7.2683e-01,\n",
            "         -1.6239e+00,  1.4518e+00, -1.1971e+00, -1.6303e+00, -7.9332e-01,\n",
            "          1.3628e+00, -2.8569e-01,  1.4890e-01,  1.3054e-01,  5.3276e-01,\n",
            "          1.6707e+00,  7.4382e-01,  4.6632e-01, -3.6223e-01, -7.6681e-03,\n",
            "          5.8753e-01, -1.3905e+00, -1.2142e+00, -5.6348e-01, -1.4227e+00,\n",
            "         -5.1937e-03, -1.4364e+00, -1.3841e+00,  6.6844e-01,  9.2323e-01,\n",
            "         -1.8478e+00, -4.8294e-01, -1.1837e+00,  1.2054e+00, -7.1639e-01,\n",
            "          1.5413e-01,  4.2904e-01, -2.7755e-01, -7.2965e-01,  8.1638e-01,\n",
            "         -2.1694e-01,  3.1080e-01,  1.0804e+00,  2.2141e+00,  7.6502e-01,\n",
            "         -8.5404e-02, -1.2318e-01, -3.7783e-01, -1.2938e+00,  1.0839e+00,\n",
            "         -2.0161e+00,  3.0724e-01,  1.1538e-01, -1.5453e+00, -4.0344e-01,\n",
            "         -4.1965e-01, -1.7416e+00,  1.0697e+00, -5.4030e-01, -7.8888e-01,\n",
            "         -1.0265e+00, -3.3806e-01,  2.6005e-02, -1.4824e-01, -1.5886e+00,\n",
            "          7.5252e-02, -1.0897e+00,  2.1909e+00,  3.1396e-01,  8.4348e-01,\n",
            "          1.2514e+00, -1.7959e+00,  2.2651e+00,  4.9332e-01, -2.8640e+00,\n",
            "         -1.1520e+00, -7.3359e-01,  9.4471e-01,  1.2581e+00, -1.3583e-02,\n",
            "         -1.4146e+00, -1.6869e-01, -1.1441e+00, -4.9102e-01,  8.9329e-02,\n",
            "          1.9948e+00, -9.5419e-01, -7.6262e-02,  5.8798e-01,  3.3358e-01,\n",
            "         -1.5716e+00, -7.7708e-01,  5.0490e-02,  1.5934e+00,  2.4076e+00,\n",
            "          1.0362e-01,  8.4523e-01,  1.6840e+00,  1.8338e+00, -7.1695e-01,\n",
            "          7.8843e-01,  1.0786e+00, -7.6690e-01, -1.2718e+00,  3.4170e+00,\n",
            "         -2.0835e+00, -2.0691e+00,  1.3800e-01,  6.4835e-01, -6.9584e-03,\n",
            "         -8.8298e-01,  2.3607e+00,  1.8832e+00,  2.9528e+00,  1.0559e+00,\n",
            "          7.7965e-01, -1.6539e+00, -5.7065e-01,  3.2931e+00, -2.5152e+00,\n",
            "          1.0171e-01, -1.4623e+00, -2.4559e-01, -1.7113e+00, -1.9348e+00,\n",
            "         -1.4530e+00, -9.0107e-01,  9.9743e-01,  6.7870e-01,  2.3216e+00,\n",
            "         -1.1447e+00, -2.5840e+00, -2.0184e-01, -9.2855e-01, -1.6589e+00,\n",
            "          1.7462e-02, -1.1543e+00, -8.3557e-01, -1.5930e-01,  8.2389e-01,\n",
            "         -6.4945e-01, -1.4111e+00,  2.1880e+00,  2.3853e-01,  1.3686e+00,\n",
            "         -1.1227e+00,  2.5330e+00, -6.8398e-02,  1.3563e+00, -1.7189e+00,\n",
            "          7.3234e-01,  8.3760e-01,  1.5083e+00,  8.2883e-01, -4.5982e-01,\n",
            "          2.7068e+00, -4.9501e-01, -3.0308e-01, -1.8209e+00,  9.4734e-01,\n",
            "          9.8267e-01,  7.0033e-01, -6.3471e-01,  1.8322e-01, -2.0834e+00,\n",
            "          2.8826e+00, -5.5186e-01, -9.0993e-01,  1.0271e+00, -6.0061e-01,\n",
            "         -5.4984e-01,  2.4920e+00, -1.0981e+00,  2.6719e+00,  2.5228e+00,\n",
            "          1.3583e+00, -2.1408e-01,  2.3641e+00,  2.6497e+00,  2.8643e-01,\n",
            "          1.3176e-01,  1.0334e+00, -6.5454e-01,  1.7773e-01,  1.1524e+00,\n",
            "         -5.6685e-01,  7.9837e-01,  1.7750e+00, -4.9195e-01, -2.3957e+00,\n",
            "         -6.6867e-01,  3.2672e-02,  3.1018e-01,  6.6509e-01,  3.2959e-01,\n",
            "          1.8881e-01,  1.0288e+00,  1.2695e+00,  7.1013e-01,  2.6245e+00,\n",
            "         -3.2817e-01, -2.7082e-01, -1.2129e-01, -1.8133e+00,  3.3645e-01,\n",
            "          2.0591e+00, -9.4264e-01,  1.6468e-01, -1.1903e+00, -1.2676e+00,\n",
            "          1.3168e+00, -1.2346e+00,  2.8132e+00, -1.2948e+00,  1.1005e+00,\n",
            "         -9.4996e-01, -1.7007e+00,  2.0181e+00, -1.3503e+00,  5.0184e-01,\n",
            "          2.7672e-01, -1.3616e+00, -4.9862e-01,  1.1553e-01, -1.0583e+00,\n",
            "         -9.4823e-01, -1.0980e+00, -6.9656e-01,  4.6512e-01,  1.2805e+00,\n",
            "         -1.7321e+00, -1.0295e+00,  1.7850e+00, -1.3765e+00,  1.3201e+00,\n",
            "          3.9022e-01,  2.2943e+00,  2.4106e+00,  1.5368e+00,  1.4140e+00,\n",
            "         -1.3643e+00,  4.2738e-01, -7.9109e-01, -1.1116e+00, -9.9195e-01,\n",
            "         -1.1517e+00, -1.7700e+00, -1.9967e-01, -4.2343e-02, -2.0305e-01,\n",
            "         -5.9415e-01,  1.5892e+00, -1.6663e+00, -5.8887e-02, -1.7874e-01,\n",
            "          1.7107e+00,  3.5350e+00, -3.4788e-01, -1.7795e+00,  5.8011e-01,\n",
            "         -1.1641e+00, -1.8110e+00, -1.3325e+00, -9.7139e-01, -1.2092e+00,\n",
            "          9.5702e-01, -2.8962e-01,  1.5868e-01, -1.5311e+00,  2.4708e+00,\n",
            "          3.0108e-03,  4.7628e-01, -1.3125e+00,  5.9602e-01,  4.5756e-01,\n",
            "          6.1955e-01,  2.0271e+00,  1.3298e+00,  1.0381e+00,  6.8159e-01,\n",
            "          1.3297e+00, -4.0751e-01,  1.2632e+00, -1.4811e-01, -1.9780e+00,\n",
            "         -9.9866e-01,  1.6414e+00, -1.1184e+00, -9.1310e-02, -1.4381e+00,\n",
            "          8.8917e-01, -3.2341e-02, -5.1443e-01, -6.5563e-01, -2.9842e+00,\n",
            "          1.8825e+00,  1.5538e+00,  3.0125e-01, -1.3184e+00,  7.9947e-01,\n",
            "         -1.0646e+00,  9.9640e-01, -7.5452e-01, -6.4958e-01,  2.7098e+00,\n",
            "          5.6363e-01,  1.8162e-01,  2.2863e+00, -2.8271e-01, -2.8810e-01,\n",
            "         -1.3207e-01, -1.3310e+00, -9.8914e-01, -8.1571e-02, -6.9299e-01,\n",
            "         -7.7003e-02,  1.1514e+00, -9.6702e-01,  3.2277e+00,  1.0919e+00,\n",
            "          8.2522e-01, -5.0172e-01,  1.1140e+00, -8.4857e-01,  1.9716e+00,\n",
            "          8.1932e-01, -6.8116e-01,  2.0773e+00, -2.5930e-01,  5.5420e-01,\n",
            "          8.2317e-01,  4.3557e-01,  2.1973e+00, -7.0327e-01, -6.3745e-01,\n",
            "          1.0598e+00,  6.2053e-01, -3.6420e-01,  2.6402e+00,  2.3878e+00,\n",
            "         -3.8503e-01, -2.5138e+00, -1.0299e+00,  1.1365e+00,  1.3322e-01,\n",
            "         -3.4515e-01, -1.0343e-01,  6.9892e-01, -4.0032e-01,  9.4668e-01,\n",
            "          1.5264e+00, -9.0285e-01,  8.5424e-01, -7.8280e-01,  3.8393e-01,\n",
            "         -1.1964e+00, -8.3332e-01, -5.6339e-01, -2.0741e+00,  1.6138e+00,\n",
            "          1.9156e+00,  3.7094e-01, -1.1170e+00, -1.3873e-01, -9.4154e-01,\n",
            "         -1.1265e+00,  2.2630e+00,  1.1529e+00,  1.8555e+00, -2.0912e-01,\n",
            "         -2.5446e-01,  1.4640e+00, -5.0461e-01,  5.1485e-01, -1.1339e+00,\n",
            "         -4.5118e-01, -6.0626e-01, -1.1714e+00, -1.8470e+00,  5.9319e-01,\n",
            "         -8.5059e-01,  1.1143e-01, -1.9150e+00,  1.6001e+00, -1.8424e+00,\n",
            "         -6.5289e-01, -1.3148e+00, -8.5787e-01,  7.2161e-01,  2.7672e-01,\n",
            "         -9.3245e-01, -9.4843e-01,  1.0827e-01, -1.0661e+00, -2.5451e-01,\n",
            "          1.1261e+00,  5.9321e-01,  1.2531e+00, -3.4615e-01,  2.5517e+00,\n",
            "          2.8885e+00,  4.6029e-01, -1.6122e+00,  7.6035e-01, -1.7514e-01,\n",
            "         -7.1376e-01, -2.9421e-01,  3.6464e+00, -6.8513e-01, -4.0046e-01,\n",
            "          1.0089e+00,  3.9204e-02, -9.5965e-01, -2.8448e-01,  5.2402e-01,\n",
            "         -5.4752e-01, -5.6688e-02,  2.3074e-01,  6.6741e-01, -1.4312e+00,\n",
            "         -1.1106e+00, -1.3358e+00, -8.9132e-01,  1.7898e+00, -1.5299e+00,\n",
            "          1.7564e+00, -2.3721e-02,  8.3979e-01, -6.9363e-01, -1.0682e+00,\n",
            "         -2.4444e-01, -7.4190e-01, -2.0704e+00, -1.6255e+00,  1.4080e+00,\n",
            "          4.4410e-01, -1.0197e+00,  1.1600e+00,  4.6482e-01, -6.8002e-01,\n",
            "          1.8921e+00, -8.5239e-01, -1.2907e+00, -1.0328e+00, -8.9167e-01,\n",
            "         -7.9051e-01, -8.8941e-01,  3.0904e+00,  1.5110e+00, -9.4637e-01,\n",
            "          1.3691e+00, -2.1957e-01, -1.1009e+00,  1.7063e+00,  1.0145e+00,\n",
            "          1.3184e+00, -1.2213e+00,  1.0198e+00, -4.3943e-01,  1.8593e+00,\n",
            "          4.4055e-01,  5.1616e-01,  1.7223e-01,  3.9129e-01,  3.4308e-02,\n",
            "          1.6196e+00,  1.8396e+00, -4.7821e-01,  7.2594e-02, -3.9053e-01,\n",
            "         -1.6355e+00, -7.8192e-01, -5.6634e-01, -2.6965e-01,  6.2045e-01,\n",
            "          1.4882e+00,  1.7509e+00,  1.5687e+00, -6.7763e-01, -5.9367e-01,\n",
            "         -7.4914e-02, -2.1820e+00, -6.0322e-01, -5.5209e-01, -8.2471e-02,\n",
            "         -1.3410e+00, -9.7352e-01,  7.0672e-01, -4.0018e-01, -7.5977e-01,\n",
            "         -2.1652e-01, -1.5341e+00, -2.1912e+00, -1.5609e+00, -9.4697e-01,\n",
            "         -1.9943e+00, -9.7593e-01, -1.8142e+00, -2.6849e-01, -1.4644e+00,\n",
            "         -4.5964e-01, -8.1065e-02, -1.6605e+00, -1.2067e-01,  8.7505e-01,\n",
            "         -3.6047e-01,  8.2617e-01, -8.1047e-02, -1.0924e+00,  1.1271e-01,\n",
            "         -1.0191e+00, -1.9878e+00, -7.5671e-01,  7.3778e-02, -1.1298e+00,\n",
            "         -1.2053e+00, -2.6540e-01, -2.0783e+00, -6.9615e-01,  2.0819e-01,\n",
            "         -1.6990e+00, -1.9576e-01, -3.6909e-01, -2.4025e-01, -7.5460e-01,\n",
            "         -1.9075e-01,  1.1125e+00,  2.7367e-01, -1.4301e+00, -2.7335e-01,\n",
            "          4.3948e-02,  5.8782e-01,  1.5867e+00,  7.2768e-01, -6.4579e-01,\n",
            "         -2.3366e-01,  1.9787e-01, -1.5141e+00, -1.7231e+00,  7.5986e-01,\n",
            "         -4.6703e-01, -1.0133e+00, -1.4813e+00, -1.0074e+00, -3.8327e-01,\n",
            "         -8.5626e-02, -1.1114e+00, -1.1304e+00, -2.4247e-01, -1.0833e-01,\n",
            "         -4.5360e-01, -1.0993e+00, -7.6691e-02, -1.8909e+00,  1.6310e+00]],\n",
            "       grad_fn=<AddmmBackward0>) torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet (residual network)\n",
        "\n",
        "Neural networks with more layers were assumed to be more effective because adding more layers improves the model performance.\n",
        "\n",
        "As the networks became deeper, the extracted features could be further enriched, such as seen with VGG16 and VGG19.\n",
        "\n",
        "A question arose: “Is learning networks as easy as stacking more layers”? An obstacle to answering this question, the gradient vanishing problem, was addressed by normalized initializations and intermediate normalization layers.\n",
        "\n",
        "However, a new issue emerged: the degradation problem. As the neural networks became deeper, accuracy saturated and degraded rapidly. An experiment comparing shallow and deep plain networks revealed that deeper models exhibited higher training and test errors, suggesting a fundamental challenge in training deeper architectures effectively. This degradation was not because of overfitting but because the training error increased when the network became deeper. The added layers did not approximate the identity function.\n",
        "\n",
        "ResNet’s residual connections unlocked the potential of the extreme depth, propelling the accuracy upwards compared to the previous architectures.\n",
        "\n",
        "![](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/ResnetBlock.png)\n",
        "\n",
        "Shortcut connections perform identity mapping and their output is added to the output of the stacked layers. Identity shortcut connections add neither extra parameters nor computational complexity, these connections bypass layers, creating direct paths for information flow, and they enable neural networks to learn the residual function (F).\n",
        "\n"
      ],
      "metadata": {
        "id": "WRvyhaB00bs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55"
      ],
      "metadata": {
        "id": "BP7GgUrE0_ll"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4KB1aXS58oV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}