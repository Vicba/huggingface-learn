{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer\n",
        "\n",
        "As the Transformers architecture scaled well in Natural Language Processing, the same architecture was applied to images by creating small patches of the image and treating them as tokens. The result was a Vision Transformer (Vision Transformers).\n",
        "\n",
        "Itâ€™s not feasible for everyone to train a Vision Transformer on millions of images to get good performance. Instead, one can use openly available models from places such as the Hugging Face Hub.\n",
        "\n",
        "What do you do with the pre-trained model? You can apply transfer learning and fine-tune it!"
      ],
      "metadata": {
        "id": "Vq0X8Wv9MetK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUAQlBkdMZCo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swin Transformer\n",
        "\n",
        "https://arxiv.org/pdf/2103.14030.pdf\n",
        "\n",
        "Swin Transformer architecture optimizes for latency and performance using a shifted window (as opposed to sliding window) approach which reduces the number of operations required. Swin is considered a hierarchical backbone for computer vision. Swin can be used for tasks like image classification.\n",
        "\n",
        "A backbone, in terms of deep learning, is a part of a neural network that does feature extraction. Additional layers can be added to the backbone to do a variety of vision tasks. Hierarchical backbones have tiered structures, sometimes with varying resolutions. This is in contrast to the non-hierarchical plain backbone in VitDet model."
      ],
      "metadata": {
        "id": "BYce27I5NpIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**shifted windows**\n",
        "\n",
        "In the original ViT, attention is done between each patch and all other patches, which gets computationally intensive. Swin optimizes this process by reducing the normally quadratic complexity ViT into linear complexity (with respect to image size). Swin achieves this using a technique similar to CNN, where patches only attend to other patches in the same window, as opposed to all other patches, and then are gradually merged with neighboring patches. This is what makes Swin a hierarchical model.\n",
        "\n",
        "![](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/swin_transformer_architecture.png)\n",
        "\n",
        "advantages:\n",
        "- computatinoal efficiency\n",
        "- large datasets: as training size goes up swin outperforms cnn."
      ],
      "metadata": {
        "id": "Zd0a4eEcN2O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## swin transformer class\n",
        "\n",
        "Swin Transformer class\n",
        "\n",
        "1. Initialize Parameters. Among various other dropout and normalization parameters, these parameters include:\n",
        "\n",
        "- window_size: Size of the windows for local self-attention.\n",
        "- ape (bool): If True, add absolute position embedding to the patch embedding.\n",
        "- fused_window_process: Optional hardware optimization\n",
        "\n",
        "2. Apply Patch Embedding: Similar to ViT, Images are split into non-overlapping patches and linearly embedded using Conv2D.\n",
        "\n",
        "3. Apply Positional Embeddings: SwinTransformer optionally uses absolute position embeddings (ape), added to the patch embeddings. Absolute positional embeddings often help the model learn to use positional information about each patch to make more informed predictions.\n",
        "\n",
        "4. Apply Depth Decay: Depth decay helps with regularization and preventing overfitting. Depth decay usually done by skipping layers during training. In this Swin implementation, stochastic depth decay is used, which means the deeper the layer, the higher the chance it might be skipped.\n",
        "\n",
        "5. Layer Construction:\n",
        "\n",
        "- The model is composed of multiple layers (BasicLayer) of SwinTransformerBlocks, each downsampling the feature map for hierarchical processing using PatchMerging.\n",
        "- The dimensionality of features and resolution of feature maps change across layers.\n",
        "\n",
        "6. Classification Head: Similar to ViT, it uses an Multi-Layer Perceptron (MLP) head for classification tasks, as defined in self.head, as the last step"
      ],
      "metadata": {
        "id": "DM096Op2Oa9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=224,\n",
        "        patch_size=4,\n",
        "        in_chans=3,\n",
        "        num_classes=1000,\n",
        "        embed_dim=96,\n",
        "        depths=[2, 2, 6, 2],\n",
        "        num_heads=[3, 6, 12, 24],\n",
        "        window_size=7,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.1,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        ape=False,\n",
        "        patch_norm=True,\n",
        "        use_checkpoint=False,\n",
        "        fused_window_process=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ape = ape\n",
        "        self.patch_norm = patch_norm\n",
        "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # split image into non-overlapping patches\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim,\n",
        "            norm_layer=norm_layer if self.patch_norm else None,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        patches_resolution = self.patch_embed.patches_resolution\n",
        "        self.patches_resolution = patches_resolution\n",
        "\n",
        "        # absolute position embedding\n",
        "        if self.ape:\n",
        "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "            trunc_normal_(self.absolute_pos_embed, std=0.02)\n",
        "\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        dpr = [\n",
        "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
        "        ]  # stochastic depth decay rule\n",
        "\n",
        "        # build layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i_layer in range(self.num_layers):\n",
        "            layer = BasicLayer(\n",
        "                dim=int(embed_dim * 2**i_layer),\n",
        "                input_resolution=(\n",
        "                    patches_resolution[0] // (2**i_layer),\n",
        "                    patches_resolution[1] // (2**i_layer),\n",
        "                ),\n",
        "                depth=depths[i_layer],\n",
        "                num_heads=num_heads[i_layer],\n",
        "                window_size=window_size,\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                drop=drop_rate,\n",
        "                attn_drop=attn_drop_rate,\n",
        "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
        "                norm_layer=norm_layer,\n",
        "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
        "                use_checkpoint=use_checkpoint,\n",
        "                fused_window_process=fused_window_process,\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.head = (\n",
        "            nn.Linear(self.num_features, num_classes)\n",
        "            if num_classes > 0\n",
        "            else nn.Identity()\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {\"absolute_pos_embed\"}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {\"relative_position_bias_table\"}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        if self.ape:\n",
        "            x = x + self.absolute_pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm(x)  # B L C\n",
        "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4HMJYR8DOZcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3h6_frVmPF7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Swin transformer block initialization\n",
        "\n",
        "The SwinTransformerBlock encapsulates the core operations of the Swin Transformer: local windowed attention and subsequent MLP processing. It plays a key role in enabling the Swin Transformer to efficiently handle large images by focusing on local patches while maintaining the ability to learn global representations.\n",
        "\n"
      ],
      "metadata": {
        "id": "yHk83X36PHCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    r\"\"\"Swin Transformer Block.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resulotion.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Window size.\n",
        "        shift_size (int): Shift size for SW-MSA.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        input_resolution,\n",
        "        num_heads,\n",
        "        window_size=7,\n",
        "        shift_size=0,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        drop=0.0,\n",
        "        attn_drop=0.0,\n",
        "        drop_path=0.0,\n",
        "        act_layer=nn.GELU,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        fused_window_process=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.window_size:\n",
        "            # if window size is larger than input resolution, we don't partition windows\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=to_2tuple(self.window_size),\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(\n",
        "            in_features=dim,\n",
        "            hidden_features=mlp_hidden_dim,\n",
        "            act_layer=act_layer,\n",
        "            drop=drop,\n",
        "        )\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            # calculate attention mask for SW-MSA\n",
        "            H, W = self.input_resolution\n",
        "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
        "            h_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            w_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    img_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "\n",
        "            mask_windows = window_partition(\n",
        "                img_mask, self.window_size\n",
        "            )  # nW, window_size, window_size, 1\n",
        "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n",
        "                attn_mask == 0, float(0.0)\n",
        "            )\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        self.register_buffer(\"attn_mask\", attn_mask)\n",
        "        self.fused_window_process = fused_window_process\n",
        "\n",
        "    ### New cell ###\n",
        "    def forward(self, x):\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            if not self.fused_window_process:\n",
        "                shifted_x = torch.roll(\n",
        "                    x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n",
        "                )\n",
        "                # partition windows\n",
        "                x_windows = window_partition(\n",
        "                    shifted_x, self.window_size\n",
        "                )  # nW*B, window_size, window_size, C\n",
        "            else:\n",
        "                x_windows = WindowProcess.apply(\n",
        "                    x, B, H, W, C, -self.shift_size, self.window_size\n",
        "                )\n",
        "        else:\n",
        "            shifted_x = x\n",
        "            # partition windows\n",
        "            x_windows = window_partition(\n",
        "                shifted_x, self.window_size\n",
        "            )  # nW*B, window_size, window_size, C\n",
        "\n",
        "        x_windows = x_windows.view(\n",
        "            -1, self.window_size * self.window_size, C\n",
        "        )  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(\n",
        "            x_windows, mask=self.attn_mask\n",
        "        )  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            if not self.fused_window_process:\n",
        "                shifted_x = window_reverse(\n",
        "                    attn_windows, self.window_size, H, W\n",
        "                )  # B H' W' C\n",
        "                x = torch.roll(\n",
        "                    shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n",
        "                )\n",
        "            else:\n",
        "                x = WindowProcessReverse.apply(\n",
        "                    attn_windows, B, H, W, C, self.shift_size, self.window_size\n",
        "                )\n",
        "        else:\n",
        "            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "        x = shortcut + self.drop_path(x)\n",
        "\n",
        "        # Feed-forward network (FFN)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BZTYCKV9PJRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Swin transformer block forward pass\n",
        "\n",
        "There are 4 key steps:\n",
        "\n",
        "1. Cyclic shift: The feature map is partitioned into windows via window_partition. A cyclic shift is then applied to the partitions. Cyclic shift is done by moving elements (in this case, partitions) in a sequence to the left or right, and wrapping around the elements that go off one end back to the other end. This process changes the positions of the elements relative to each other but keeps the sequence otherwise intact. For example, if you cyclically shift the sequence A, B, C, D to the right by one position, it becomes D, A, B, C.\n",
        "\n",
        "Cyclic shift allows the model to capture relationships between adjacent windows, enhancing its ability to learn spatial contexts beyond the local scope of individual windows.\n",
        "\n",
        "2. Windowed attention: Perform attention using window-based multi-head self attention (W-MSA) module\n",
        "\n",
        "3. Merge Patches: Patches are merged via PatchMerging\n",
        "\n",
        "4. Reverse cyclic shift: After attention is done, the window partitioning is undone via reverse_window, and the cyclic shift operation is reversed, so that the feature map retains its original form."
      ],
      "metadata": {
        "id": "OAucMLEfPYDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        window_size,\n",
        "        num_heads,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        attn_drop=0.0,\n",
        "        proj_drop=0.0,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim**-0.5\n",
        "\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = (\n",
        "            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        )  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = (\n",
        "            self.qkv(x)\n",
        "            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
        "            .permute(2, 0, 3, 1, 4)\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[0],\n",
        "            qkv[1],\n",
        "            qkv[2],\n",
        "        )  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = q @ k.transpose(-2, -1)\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[\n",
        "            self.relative_position_index.view(-1)\n",
        "        ].view(\n",
        "            self.window_size[0] * self.window_size[1],\n",
        "            self.window_size[0] * self.window_size[1],\n",
        "            -1,\n",
        "        )  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(\n",
        "            2, 0, 1\n",
        "        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n",
        "                1\n",
        "            ).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rqCcGgAmPl62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### window attention\n",
        "\n",
        "WindowAttention is a window-based multi-head self attention (W-MSA) module with relative position bias. This can be used for both shifted and non-shifted windows.\n",
        "\n"
      ],
      "metadata": {
        "id": "_9OcjZ1WP1aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    r\"\"\"Patch Merging Layer.\n",
        "\n",
        "    Args:\n",
        "        input_resolution (tuple[int]): Resolution of input feature.\n",
        "        dim (int): Number of input channels.\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.input_resolution = input_resolution\n",
        "        self.dim = dim\n",
        "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
        "        self.norm = norm_layer(4 * dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: B, H*W, C\n",
        "        \"\"\"\n",
        "        H, W = self.input_resolution\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
        "\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
        "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
        "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "CpV33WJ3P3oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### patch merging layer\n",
        "\n",
        "Patch merging method is used for downsampling. It is used to reduce the spatial dimensions of the feature map, similar to pooling in traditional convolutional neural networks (CNNs). It helps in building hierarchical feature representations by progressively increasing the receptive field and reducing the spatial resolution.\n",
        "\n"
      ],
      "metadata": {
        "id": "fZyb0zYgP6MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoImageProcessor, SwinForImageClassification\n",
        "import torch\n",
        "\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
        ")\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"huggingface/cats-image\")\n",
        "image = dataset[\"test\"][\"image\"][0]\n",
        "\n",
        "inputs = image_processor(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_label_id = logits.argmax(-1).item()\n",
        "predicted_label_text = model.config.id2label[predicted_label_id]\n",
        "\n",
        "print(predicted_label_text)"
      ],
      "metadata": {
        "id": "ZbtlpbTgP4xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how to use Swin model to classify a cat image into one of the 1,000 ImageNet classes:\n",
        "\n"
      ],
      "metadata": {
        "id": "m833vx4PQEKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoImageProcessor, SwinForImageClassification\n",
        "import torch\n",
        "\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
        ")\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\"\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"huggingface/cats-image\")\n",
        "image = dataset[\"test\"][\"image\"][0]\n",
        "\n",
        "inputs = image_processor(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_label_id = logits.argmax(-1).item()\n",
        "predicted_label_text = model.config.id2label[predicted_label_id]\n",
        "\n",
        "print(predicted_label_text)"
      ],
      "metadata": {
        "id": "uGlUHrcWQFW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}