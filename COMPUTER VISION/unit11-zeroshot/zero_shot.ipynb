{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## What is Zero-shot Learning?\n",
        "\n",
        "Let’s warm up with a definition. Zero-shot learning is a setup in which the model is presented with images belonging to only classes that it was not exposed to during training, at test time. In other words, the training and testing sets are disjoint. Just a heads-up: in the classic ZSL setup, the test set only has pictures of classes the model hasn’t seen before, not a single one from its training days. This may seem a little bit unrealistic, it’s like asking a student to ace an exam on only materials they’ve never studied. Luckily, there’s a more pragmatic version of ZSL that doesn’t have this strict rule and is called generalized zero-shot learning, or GZSL. This more flexible approach allows the test set to include both seen and unseen classes. It’s a more realistic scenario, reflecting how things work in the real world.\n",
        "\n",
        "Zero-shot learning in computer vision allows models to recognize objects they haven't seen before, but it differs from the more straightforward approach in NLP. While language models can perform tasks they weren’t explicitly trained on by learning patterns in large text datasets, zero-shot learning in computer vision relies on *multi-modal* information.\n",
        "\n",
        "Humans can recognize new objects by comparing them to known ones, like identifying a zebra as a striped horse. Similarly, zero-shot models use descriptive information about unseen objects to generalize. This involves associating features of new objects with existing knowledge, making it different from unsupervised learning. Though not trained directly on unseen object data, the model still relies on labeled descriptions, hence it's still a form of supervised learning, known initially as \"dataless classification.\""
      ],
      "metadata": {
        "id": "78i1siPTNSQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-shot computer vision requires more than just visual features to recognize unseen objects**; it uses *semantic or auxiliary information* to bridge the gap between known and unknown classes. This semantic information comes in forms like attribute vectors, textual descriptions, and class label embeddings. The model learns to map image features to these semantic features, and at inference, predicts new classes by finding the closest match in the semantic space, often using methods like k-nearest neighbor.\n",
        "\n",
        "Zero-shot learning is a type of *heterogeneous transfer learning*, where knowledge from seen classes (source domain) is transferred to unseen classes (target domain), overcoming challenges like domain shift and lack of labeled data.\n",
        "\n",
        "Two main approaches in zero-shot computer vision are:\n",
        "1. **Embedding-based methods**: Both images and their semantic features are projected into a shared embedding space, and unseen classes are predicted using similarity measures (e.g., CLIP).\n",
        "2. **Generative-based methods**: These methods use generative models to create synthetic data for unseen classes, transforming the zero-shot problem into a supervised learning one (e.g., CVAE).\n",
        "\n",
        "Embedding-based methods are efficient and scalable, while generative-based methods offer more flexibility. This discussion focuses on embedding-based techniques."
      ],
      "metadata": {
        "id": "L9-A8fzxOmSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlvFPcp2MjAd"
      },
      "outputs": [],
      "source": []
    }
  ]
}