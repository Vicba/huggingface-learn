{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# StyleGAN\n",
        "\n",
        "Generative Adversarial Networks (GANs) are a class of generative models designed to produce realistic images. In Vanilla GANs, there are two networks: a Generator, which creates images from a noise vector, and a Discriminator, which determines whether an image is real or generated. While initially the Generator produces low-quality images, its goal is to eventually create images that can fool the Discriminator.\n",
        "\n",
        "However, Vanilla GANs lack explicit control over image generation, making it challenging to generate specific features (e.g., a female wearing glasses). This is due to the entangled nature of the features in the generated images.\n",
        "\n",
        "**TL;DR**: StyleGAN is a modification of the Generator's architecture in GANs, allowing users to have control over both high-level attributes (like pose and expression) and low-level features (like skin pores and hair placement). This flexibility has enabled StyleGAN to be used in various applications, including privacy preservation and image editing."
      ],
      "metadata": {
        "id": "VahHqCIqx-ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## stylegan 1: components and benefits\n",
        "\n",
        "![](https://huggingface.co/datasets/hwaseem04/Documentation-files/resolve/main/CV-Course/stylegan_arch.png)\n",
        "\n",
        "\n",
        "1. **ProgressiveGAN Overview**: The diagram initially illustrates ProgressiveGAN, a variation of Vanilla GAN that generates images progressively at increasing resolutions, starting from 4x4 pixels and eventually producing higher-resolution images.\n",
        "\n",
        "2. **StyleGAN Architecture**: The proposed StyleGAN architecture includes three main components:\n",
        "   - **Mapping Network**: This network maps the latent code (noise vector) \\( z \\) into a disentangled latent space \\( w \\) using eight MLP layers. This mapping allows for fine control over specific features. When adjusting one feature in \\( w \\), only the corresponding real-world characteristic (e.g., smile) should change in the generated image. The mapped \\( w \\) is passed to each block of the Synthesis Network, allowing control over high-level features (like pose and hairstyle) in lower blocks and fine details (like eye openness) in higher blocks.\n",
        "  \n",
        "   - **Adaptive Instance Normalization (AdaIN)**: AdaIN adjusts the normalization parameters (mean and standard deviation) dynamically based on style information from the latent code \\( w \\). Instead of directly passing \\( w \\) to the Synthesis Network, a transformed version \\( y \\) is used to modulate the generation process, allowing for varied styles in different parts of the output.\n",
        "\n",
        "   - **Concatenation of Noise Vector**: In traditional GANs, the generator struggles to learn stochastic features (like hair position or skin pores) independently. StyleGAN improves this by adding a noise map to the feature map at each synthesis block. This approach allows each layer to utilize the noise information, leading to more diverse and detailed stochastic features in the generated images.\n"
      ],
      "metadata": {
        "id": "C9uYLXQuyUmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Cases\n",
        "\n",
        "StyleGAN’s ability to generate photorealistic images has opened doors for diverse applications, including image editing, preserving privacy, and even creative exploration.\n",
        "\n",
        "Image Editing\n",
        "- Image inpainting: Filling in missing image regions in a seamless and realistic manner.\n",
        "- Image style transfer: Transferring the style of one image to another.\n",
        "\n",
        "Privacy-preserving applications\n",
        "- Generating synthetic data: Replacing sensitive information with realistic synthetic data for training and testing purposes.\n",
        "- Anonymizing images: Blurring or altering identifiable features in images to protect individuals’ privacy.\n",
        "\n",
        "Creative explorations\n",
        "- Generating fashion designs: StyleGAN can be used to generate realistic and diverse fashion designs\n",
        "- Creating immersive experiences: StyleGAN can be used to create realistic virtual environments for gaming, education, and other applications. For instance, Stylenerf: A style-based. 3d aware generator for high-resolution image synthesis.\n",
        "\n",
        "These are just a non-exhaustive list."
      ],
      "metadata": {
        "id": "lwkVDkRDzgwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhQSDEvpMa85"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CycleGAN\n",
        "\n",
        "This section introduces CycleGAN (Cycle-Consistent Generative Adversarial Network), a framework for image-to-image translation tasks that do not require paired examples. Developed by Zhu et al. in 2017, CycleGAN significantly advances the fields of computer vision and machine learning. Traditional image-to-image translation methods often depend on large datasets of paired examples, which can be difficult to obtain. CycleGAN addresses this challenge by effectively operating with unpaired datasets."
      ],
      "metadata": {
        "id": "NCQs-O5tvDmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### unpaired image-to-image translation\n",
        "\n",
        "![](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/unpaired_images.png)\n",
        "\n",
        "In many image translation scenarios, datasets often lack direct one-to-one correspondence between image pairs, leading to the concept of unpaired image-to-image translation. Instead of matching pairs, this approach uses two distinct sets of images, each representing a different style or domain, such as realistic photographs versus artworks or winter landscapes versus summer scenes. The goal is for the model to learn and extract the general stylistic elements from each set and apply these styles to transform images from one domain to another, enabling bidirectional translation. This method is particularly useful when exact image pairs are unavailable or hard to obtain."
      ],
      "metadata": {
        "id": "M4hGGF_ovOL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main components\n",
        "\n",
        "### Main Components of CycleGAN\n",
        "\n",
        "1. **Dual GAN Structure**: CycleGAN uses two Generative Adversarial Networks (GANs)—one for translating images from the first domain to the second (e.g., zebra to horse) and another for the reverse process. This structure ensures both realism through adversarial training and content preservation via cycle consistency.\n",
        "\n",
        "2. **PatchGAN Discriminators**: The discriminators operate on patches of images instead of the entire image, allowing for more detailed and localized realism.\n",
        "\n",
        "3. **Generator Architecture**: CycleGAN generators are inspired by U-Net and DCGAN architectures, incorporating downsampling (encoding), upsampling (decoding), and convolutional layers with batch normalization and ReLU. They utilize additional convolutional layers and skip (residual) connections to facilitate learning identity functions and support deeper transformations.\n",
        "\n",
        "4. **Cycle Consistency Loss**: This loss function ensures that when an image (e.g., a sad face) is transformed to a different style (e.g., a hugging face) and then reverted back, it retains minimal detail loss. The model aims for the final reverted image to closely resemble the original, minimizing pixel differences, which is included in the overall loss function.\n",
        "\n",
        "5. **Integration with Adversarial Loss**: Cycle consistency loss is combined with adversarial loss to create a comprehensive loss function, which is optimized simultaneously for both generators.\n",
        "\n",
        "6. **Least-Square Loss**: This approach minimizes the sum of squared residuals to address issues like vanishing gradients and mode collapse that are common in binary cross-entropy loss. It measures the discrepancy between the discriminator's predictions and actual labels (real or fake), encouraging the generator to produce outputs that appear realistic.\n",
        "\n",
        "7. **Identity Loss**: This optional loss term enhances color preservation by ensuring that an image input into a generator (e.g., a horse image into a zebra-to-horse generator) outputs the same image if it’s already in the target style. The loss is calculated as the pixel distance between the input and output images, with a zero distance indicating no change, which is the desired outcome. This loss is included alongside adversarial and cycle consistency losses, adjusted using a weighting factor (lambda).\n",
        "\n",
        "![](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/cycleGAN1.jpg)\n",
        "\n",
        "This figure illustrates the functionality of the combined GAN architecture in CycleGAN, which integrates two GANs linked by cycle consistency to form a cycle. Real images are represented with ones in the classification matrix, while fake images are denoted by zeros. Overall, CycleGAN effectively transfers styles between two domains by combining multiple loss functions, including adversarial loss, cycle consistency loss, and optional identity loss, while preserving the essential characteristics of the input images."
      ],
      "metadata": {
        "id": "BybK3bAEvY8I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fc7_KOkivMLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}