{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What is a video?\n",
        "\n",
        "An image is a binary, two-dimensional (2D) representation of visual data. A video is a multimedia format that sequentially displays these frames or images.\n",
        "\n",
        "Technically speaking, the frames are separate pictures. As a result, storing and playing these frames sequentially at a conventional speed results in the creation of a video, thus giving the illusion of motion (just like a flipbook). It is a popular and widely used medium for communicating information, entertainment, and conversation. Videos and photos are obtained via image-acquisition equipment such as video cameras, smartphones, and so on.\n",
        "\n",
        "## Aspects of a Video\n",
        "\n",
        "- **Resolution:** The resolution of a video refers to the number of pixels in each frame or we can also refer to it as the size of each frame in the video. It doesn’t need to be a standard size, but there are common sizes for video. Common video resolutions include HD (1280x720 pixels), Full HD (1920x1080 pixels), Ultra HD or 4K (3840x2160 pixels), and so on. When a video is said to have a resolution of 1920x1080 pixels, it essentially means the video has a width of 1920 pixels and a height of 1080 pixels. Higher resolution videos have more detail but also require more storage space and processing power.\n",
        "\n",
        "- **Frame Rate:** A video is composed of multiple separate frames, or images. In order to give the impression of motion, these frames are displayed quickly one after the other. The number of frames displayed per second is called the “frame rate.” Common frame rates include 24, 30, and 60 frames per second (fps) or hertz (general unit for frequency). Higher frame rates result in smoother motion.\n",
        "\n",
        "- **Bitrate:** The quantity of data needed to describe audio and video is called bitrate. Better quality is achieved at higher bitrates, but streaming requires more storage and bandwidth.\n",
        "\n",
        "Bitrates for videos are commonly expressed in megabytes per second (mbps) or kilobytes per second (kbps).\n",
        "\n",
        "- **Codecs:** Codecs, short for “compressor-decompressor” are software or hardware components that compress and decompress digital media to reduce the size of media files, making them more manageable for storage and transmission while maintaining an acceptable level of quality. There are two main types of codecs; “lossless codecs” and “lossy codecs”. Lossless codecs are designed to compress data without any loss of quality, while lossy codecs are more designed to compress by removing some of the data resulting in a loss of quality.\n",
        "\n",
        "In summary, a video is a dynamic multimedia format that combines a series of individual frames, audio, and often additional metadata."
      ],
      "metadata": {
        "id": "YemC8JXg8AQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video processing basics\n",
        "\n",
        "Vision transformers are great at computer vision tasks involving both images and videos.\n"
      ],
      "metadata": {
        "id": "_OCE2bOc8AOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Image Processing for Vision Transformers\n",
        "\n",
        "When it comes to image data, vision transformers typically process individual still images by splitting them into non-overlapping patches and then transforming these patches individually. Suppose we have a 224x224 image, split the image into 16x16 patches where each patch consists of 14x14 pixels. This patch-based processing not only reduces computation but it also allows the model to capture local features in the image effectively. Each patch is then fed through a series of self-attention layers and feed-forward neural networks to extract semantic information. Thanks to this hierarchical processing technique, vision transformers are able to capture both high-level and low-level features in the image. Since vision transformers process patches individually and transformers by default don’t have any mechanism to track position of inputs, the spatial context of the image can be lost. To address this, vision transformers often include positional encodings that capture the relative position of each patch within the image. By incorporating positional information, the model can better understand the spatial relationships between different patches and enhance its ability to recognize objects and patterns.\n",
        "\n",
        "**Note: CNNs are designed to learn spatial features, while vision transformers are designed to learn both spatial and contextual features.**\n",
        "\n",
        "## Key Differences Between Image and Video Processing\n",
        "\n",
        "Videos are essentially a sequence of frames, and processing them requires techniques to capture and incorporate motion information. In image processing, the transformer ignores the temporal (time) relations between frames, i.e., it only focuses on a frame’s spatial (space) information.\n",
        "\n",
        "Temporal relations are the main factors for developing a strong understanding of content in a video, thus we require a separate algorithm for videos. **One of the main differences between image and video processing is the inclusion of an additional axis, time to the input. There are two main approaches for extracting tokens from a video or embedding a video clip.**\n",
        "\n",
        "### 1. Uniform Frame Sampling\n",
        "\n",
        "It is a straightforward method of tokenizing the input video in which we uniformly sample $ n_t $ frames from the input video clip, embed each 2D frame independently using the same method as used in image processing, and concatenate all these tokens together.\n",
        "\n",
        "If $ n_h \\times n_w $ non-overlapping image patches are extracted from each frame, then a total of $ n_t \\times n_h \\times n_w' $ tokens will be forwarded through the transformer encoder. Uniform frame sampling is a tokenization scheme in which we sample frames from the video clip and perform simple ViT tokenization.\n",
        "\n",
        "### 2. Tubelet Embedding\n",
        "\n",
        "This method extends the vision transformer’s image embedding to 3D and corresponds to a 3D convolution. It is an alternative method in which non-overlapping, spatiotemporal “tubes” from input volume are extracted and linearly projected.\n",
        "\n",
        "First, we extract tubes from the video. These tubes contain patches of the frame and the temporal information as well. The tubes are then flattened to build video tokens. Intuitively, this method fuses spatiotemporal information during tokenization, in contrast to “uniform frame sampling”, where temporal information from different frames is fused by the transformer.\n",
        "\n",
        "## Importance of Temporal Information in Video Processing\n",
        "\n",
        "The inclusion of temporal information in video processing is crucial for several computer vision tasks. One such task is action recognition, which aims to classify the action in a video. Temporal information is also essential for tasks like video captioning, where the goal is to generate a textual description of the content in a video.\n",
        "\n",
        "By considering the temporal relationships between frames, vision transformers can generate more contextually relevant captions. For example, if a person is shown running in one frame and then jumping in the next, the model can generate a caption that reflects this sequence of action. Furthermore, temporal processing is important for tasks like video object detection and tracking.\n",
        "\n",
        "In conclusion, the presence of temporal information and the particular difficulties posed by video data, such as higher memory and storage needs, are the main processing distinctions between video and image. The choice between image and video processing depends on the specific computer vision task and the characteristics of the data."
      ],
      "metadata": {
        "id": "qtp0QiPL8x6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30hULLpw5AWy"
      },
      "outputs": [],
      "source": []
    }
  ]
}